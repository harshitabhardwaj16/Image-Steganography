{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of beta=0.6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1KoiZ5KJBYt"
      },
      "source": [
        "# Imports necessary libraries and modules\n",
        "from itertools import islice\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os \n",
        "import pickle\n",
        "from torchvision import datasets, utils\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "from random import shuffle\n",
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrO01lBEJhnG"
      },
      "source": [
        "# Directory path\n",
        "os.chdir(\"..\")\n",
        "cwd = 'content/drive/My Drive/'\n",
        "\n",
        "# Hyper Parameters\n",
        "num_epochs = 3\n",
        "batch_size = 8\n",
        "learning_rate = 0.0001\n",
        "beta = 0.75\n",
        "\n",
        "# Mean and std deviation of imagenet dataset. Source: http://cs231n.stanford.edu/reports/2017/pdfs/101.pdf\n",
        "std = [0.229, 0.224, 0.225]\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "\n",
        "# TODO: Define train, validation and models\n",
        "MODELS_PATH = 'drive/'\n",
        "# TRAIN_PATH = cwd+'/train/'\n",
        "# VALID_PATH = cwd+'/valid/'\n",
        "VALID_PATH = cwd+''\n",
        "TRAIN_PATH = cwd+''\n",
        "TEST_PATH = cwd+''\n",
        "\n",
        "if not os.path.exists(MODELS_PATH): os.mkdir(MODELS_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEQOY8CxJh3-"
      },
      "source": [
        "def customized_loss(S_prime, C_prime, S, C, B):\n",
        "    ''' Calculates loss specified on the paper.'''\n",
        "    \n",
        "    loss_cover = torch.nn.functional.mse_loss(C_prime, C)\n",
        "    loss_secret = torch.nn.functional.mse_loss(S_prime, S)\n",
        "    loss_all = loss_cover + B * loss_secret\n",
        "    return loss_all, loss_cover, loss_secret\n",
        "\n",
        "def denormalize(image, std, mean):\n",
        "    ''' Denormalizes a tensor of images.'''\n",
        "\n",
        "    for t in range(3):\n",
        "        image[t, :, :] = (image[t, :, :] * std[t]) + mean[t]\n",
        "    return image\n",
        "\n",
        "def imshow(img, idx, learning_rate, beta):\n",
        "    '''Prints out an image given in tensor format.'''\n",
        "    \n",
        "    img = denormalize(img, std, mean)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.title('Example '+str(idx)+', lr='+str(learning_rate)+', B='+str(beta))\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def gaussian(tensor, mean=0, stddev=0.1):\n",
        "    '''Adds random noise to a tensor.'''\n",
        "    \n",
        "    noise = torch.nn.init.normal(torch.Tensor(tensor.size()), 0, 0.1)\n",
        "    return Variable(tensor + noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FtBli9XJh9C"
      },
      "source": [
        "# Preparation Network (2 conv layers)\n",
        "class PrepNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrepNetwork, self).__init__()\n",
        "        self.initialP3 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.initialP4 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.initialP5 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalP3 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.finalP4 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalP5 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "\n",
        "    def forward(self, p):\n",
        "        p1 = self.initialP3(p)\n",
        "        p2 = self.initialP4(p)\n",
        "        p3 = self.initialP5(p)\n",
        "        mid = torch.cat((p1, p2, p3), 1)\n",
        "        p4 = self.finalP3(mid)\n",
        "        p5 = self.finalP4(mid)\n",
        "        p6 = self.finalP5(mid)\n",
        "        out = torch.cat((p4, p5, p6), 1)\n",
        "        return out\n",
        "\n",
        "# Hiding Network (5 conv layers)\n",
        "class HidingNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HidingNetwork, self).__init__()\n",
        "        self.initialH3 = nn.Sequential(\n",
        "            nn.Conv2d(153, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.initialH4 = nn.Sequential(\n",
        "            nn.Conv2d(153, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.initialH5 = nn.Sequential(\n",
        "            nn.Conv2d(153, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalH3 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.finalH4 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalH5 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalH = nn.Sequential(\n",
        "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
        "        \n",
        "    def forward(self, h):\n",
        "        h1 = self.initialH3(h)\n",
        "        h2 = self.initialH4(h)\n",
        "        h3 = self.initialH5(h)\n",
        "        mid = torch.cat((h1, h2, h3), 1)\n",
        "        h4 = self.finalH3(mid)\n",
        "        h5 = self.finalH4(mid)\n",
        "        h6 = self.finalH5(mid)\n",
        "        mid2 = torch.cat((h4, h5, h6), 1)\n",
        "        out = self.finalH(mid2)\n",
        "        out_noise = gaussian(out.data, 0, 0.1)\n",
        "        return out, out_noise\n",
        "\n",
        "# Reveal Network (2 conv layers)\n",
        "class RevealNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RevealNetwork, self).__init__()\n",
        "        self.initialR3 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.initialR4 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.initialR5 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalR3 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.finalR4 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalR5 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalR = nn.Sequential(\n",
        "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
        "\n",
        "    def forward(self, r):\n",
        "        r1 = self.initialR3(r)\n",
        "        r2 = self.initialR4(r)\n",
        "        r3 = self.initialR5(r)\n",
        "        mid = torch.cat((r1, r2, r3), 1)\n",
        "        r4 = self.finalR3(mid)\n",
        "        r5 = self.finalR4(mid)\n",
        "        r6 = self.finalR5(mid)\n",
        "        mid2 = torch.cat((r4, r5, r6), 1)\n",
        "        out = self.finalR(mid2)\n",
        "        return out\n",
        "\n",
        "# Join three networks in one module\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.m1 = PrepNetwork()\n",
        "        self.m2 = HidingNetwork()\n",
        "        self.m3 = RevealNetwork()\n",
        "\n",
        "    def forward(self, secret, cover):\n",
        "        x_1 = self.m1(secret)\n",
        "        mid = torch.cat((x_1, cover), 1)\n",
        "        x_2, x_2_noise = self.m2(mid)\n",
        "        x_3 = self.m3(x_2_noise)\n",
        "        return x_2, x_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLsI5Z_NJiBx"
      },
      "source": [
        "# Creates net object\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gCyEB7sJiGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696b8a58-a3a9-4dc3-98ca-5f1e13e8db55"
      },
      "source": [
        "# Creates training set\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.ImageFolder(\n",
        "        TRAIN_PATH,\n",
        "        transforms.Compose([\n",
        "        transforms.Scale(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean,\n",
        "        std=std)\n",
        "        ])), batch_size=batch_size, num_workers=1, \n",
        "        pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "# Creates test set\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.ImageFolder(\n",
        "        TEST_PATH, \n",
        "        transforms.Compose([\n",
        "        transforms.Scale(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean,\n",
        "        std=std)\n",
        "        ])), batch_size=2, num_workers=1, \n",
        "        pin_memory=True, shuffle=True, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNOgrvfxJiL7"
      },
      "source": [
        "def train_model(train_loader, beta, learning_rate):\n",
        "    \n",
        "    # Save optimizer\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    loss_history = []\n",
        "    # Iterate over batches performing forward and backward passes\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Train mode\n",
        "        net.train()\n",
        "        \n",
        "        train_losses = []\n",
        "        # Train one epoch\n",
        "        for idx, train_batch in enumerate(train_loader):\n",
        "\n",
        "            data, _  = train_batch\n",
        "\n",
        "            # Saves secret images and secret covers\n",
        "            train_covers = data[:len(data)//2]\n",
        "            train_secrets = data[len(data)//2:]\n",
        "            \n",
        "            # Creates variable from secret and cover images\n",
        "            train_secrets = Variable(train_secrets, requires_grad=False)\n",
        "            train_covers = Variable(train_covers, requires_grad=False)\n",
        "\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            train_hidden, train_output = net(train_secrets, train_covers)\n",
        "\n",
        "            # Calculate loss and perform backprop\n",
        "            train_loss, train_loss_cover, train_loss_secret = customized_loss(train_output, train_hidden, train_secrets, train_covers, beta)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Saves training loss\n",
        "            train_losses.append(train_loss.item())\n",
        "            loss_history.append(train_loss.item())\n",
        "            \n",
        "            # Prints mini-batch losses\n",
        "            print('Training: Batch {0}/{1}. Loss of {2:.4f}, cover loss of {3:.4f}, secret loss of {4:.4f}'.format(idx+1, len(train_loader), train_loss.item(), train_loss_cover.item(), train_loss_secret.item()))\n",
        "    \n",
        "        torch.save(net.state_dict(), MODELS_PATH+'Epoch N{}.pkl'.format(epoch+1))\n",
        "        \n",
        "        mean_train_loss = np.mean(train_losses)\n",
        "    \n",
        "        # Prints epoch average loss\n",
        "        print ('Epoch [{0}/{1}], Average_loss: {2:.4f}'.format(\n",
        "                epoch+1, num_epochs, mean_train_loss))\n",
        "    \n",
        "    return net, mean_train_loss, loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEE-8WiQJiQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b981f0c-434d-4c28-a47f-37cc399aefe5"
      },
      "source": [
        "\n",
        "net, mean_train_loss, loss_history = train_model(train_loader, beta, learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training: Batch 1/1146. Loss of 1.4528, cover loss of 0.9826, secret loss of 0.6269\n",
            "Training: Batch 2/1146. Loss of 2.5988, cover loss of 1.4015, secret loss of 1.5965\n",
            "Training: Batch 3/1146. Loss of 2.3945, cover loss of 1.7513, secret loss of 0.8577\n",
            "Training: Batch 4/1146. Loss of 2.6087, cover loss of 1.7954, secret loss of 1.0844\n",
            "Training: Batch 5/1146. Loss of 1.8850, cover loss of 0.9606, secret loss of 1.2325\n",
            "Training: Batch 6/1146. Loss of 2.1237, cover loss of 0.9287, secret loss of 1.5933\n",
            "Training: Batch 7/1146. Loss of 2.1715, cover loss of 1.1774, secret loss of 1.3255\n",
            "Training: Batch 8/1146. Loss of 2.5340, cover loss of 1.4856, secret loss of 1.3978\n",
            "Training: Batch 9/1146. Loss of 1.5814, cover loss of 0.8359, secret loss of 0.9941\n",
            "Training: Batch 10/1146. Loss of 2.5353, cover loss of 1.7083, secret loss of 1.1027\n",
            "Training: Batch 11/1146. Loss of 2.0940, cover loss of 0.9046, secret loss of 1.5858\n",
            "Training: Batch 12/1146. Loss of 2.1422, cover loss of 1.0012, secret loss of 1.5213\n",
            "Training: Batch 13/1146. Loss of 2.7843, cover loss of 1.4234, secret loss of 1.8145\n",
            "Training: Batch 14/1146. Loss of 1.6698, cover loss of 0.7047, secret loss of 1.2868\n",
            "Training: Batch 15/1146. Loss of 1.2218, cover loss of 0.5612, secret loss of 0.8808\n",
            "Training: Batch 16/1146. Loss of 1.8649, cover loss of 0.6728, secret loss of 1.5896\n",
            "Training: Batch 17/1146. Loss of 2.0536, cover loss of 1.1064, secret loss of 1.2629\n",
            "Training: Batch 18/1146. Loss of 1.7224, cover loss of 0.7525, secret loss of 1.2931\n",
            "Training: Batch 19/1146. Loss of 2.5492, cover loss of 0.6464, secret loss of 2.5371\n",
            "Training: Batch 20/1146. Loss of 1.7208, cover loss of 0.4346, secret loss of 1.7148\n",
            "Training: Batch 21/1146. Loss of 0.9142, cover loss of 0.2312, secret loss of 0.9107\n",
            "Training: Batch 22/1146. Loss of 1.3112, cover loss of 0.3256, secret loss of 1.3142\n",
            "Training: Batch 23/1146. Loss of 1.7047, cover loss of 0.6427, secret loss of 1.4160\n",
            "Training: Batch 24/1146. Loss of 1.2972, cover loss of 0.5342, secret loss of 1.0173\n",
            "Training: Batch 25/1146. Loss of 1.1835, cover loss of 0.4216, secret loss of 1.0159\n",
            "Training: Batch 26/1146. Loss of 1.0917, cover loss of 0.1860, secret loss of 1.2076\n",
            "Training: Batch 27/1146. Loss of 1.0206, cover loss of 0.3444, secret loss of 0.9017\n",
            "Training: Batch 28/1146. Loss of 0.9704, cover loss of 0.1711, secret loss of 1.0658\n",
            "Training: Batch 29/1146. Loss of 1.2695, cover loss of 0.2292, secret loss of 1.3871\n",
            "Training: Batch 30/1146. Loss of 0.7901, cover loss of 0.2515, secret loss of 0.7182\n",
            "Training: Batch 31/1146. Loss of 1.6549, cover loss of 0.3956, secret loss of 1.6790\n",
            "Training: Batch 32/1146. Loss of 1.3807, cover loss of 0.3726, secret loss of 1.3442\n",
            "Training: Batch 33/1146. Loss of 1.5985, cover loss of 0.2184, secret loss of 1.8401\n",
            "Training: Batch 34/1146. Loss of 1.2747, cover loss of 0.4165, secret loss of 1.1442\n",
            "Training: Batch 35/1146. Loss of 1.2761, cover loss of 0.3325, secret loss of 1.2581\n",
            "Training: Batch 36/1146. Loss of 1.3997, cover loss of 0.3129, secret loss of 1.4490\n",
            "Training: Batch 37/1146. Loss of 1.0666, cover loss of 0.2990, secret loss of 1.0235\n",
            "Training: Batch 38/1146. Loss of 1.0700, cover loss of 0.3309, secret loss of 0.9855\n",
            "Training: Batch 39/1146. Loss of 1.3469, cover loss of 0.2029, secret loss of 1.5254\n",
            "Training: Batch 40/1146. Loss of 1.4372, cover loss of 0.4794, secret loss of 1.2770\n",
            "Training: Batch 41/1146. Loss of 1.3820, cover loss of 0.4326, secret loss of 1.2659\n",
            "Training: Batch 42/1146. Loss of 1.3563, cover loss of 0.2929, secret loss of 1.4180\n",
            "Training: Batch 43/1146. Loss of 0.8485, cover loss of 0.3784, secret loss of 0.6269\n",
            "Training: Batch 44/1146. Loss of 1.5227, cover loss of 0.4292, secret loss of 1.4581\n",
            "Training: Batch 45/1146. Loss of 0.8864, cover loss of 0.2701, secret loss of 0.8218\n",
            "Training: Batch 46/1146. Loss of 1.6391, cover loss of 0.2802, secret loss of 1.8119\n",
            "Training: Batch 47/1146. Loss of 1.0018, cover loss of 0.1429, secret loss of 1.1452\n",
            "Training: Batch 48/1146. Loss of 0.8037, cover loss of 0.2533, secret loss of 0.7338\n",
            "Training: Batch 49/1146. Loss of 0.7077, cover loss of 0.1660, secret loss of 0.7223\n",
            "Training: Batch 50/1146. Loss of 1.5130, cover loss of 0.2865, secret loss of 1.6353\n",
            "Training: Batch 51/1146. Loss of 1.1516, cover loss of 0.4163, secret loss of 0.9804\n",
            "Training: Batch 52/1146. Loss of 1.3552, cover loss of 0.2558, secret loss of 1.4659\n",
            "Training: Batch 53/1146. Loss of 1.2827, cover loss of 0.2381, secret loss of 1.3927\n",
            "Training: Batch 54/1146. Loss of 1.1102, cover loss of 0.2936, secret loss of 1.0889\n",
            "Training: Batch 55/1146. Loss of 1.5639, cover loss of 0.1325, secret loss of 1.9084\n",
            "Training: Batch 56/1146. Loss of 1.5237, cover loss of 0.2405, secret loss of 1.7109\n",
            "Training: Batch 57/1146. Loss of 0.7143, cover loss of 0.1301, secret loss of 0.7789\n",
            "Training: Batch 58/1146. Loss of 1.5942, cover loss of 0.5977, secret loss of 1.3286\n",
            "Training: Batch 59/1146. Loss of 1.4861, cover loss of 0.2421, secret loss of 1.6587\n",
            "Training: Batch 60/1146. Loss of 1.1875, cover loss of 0.1284, secret loss of 1.4122\n",
            "Training: Batch 61/1146. Loss of 0.9387, cover loss of 0.1578, secret loss of 1.0412\n",
            "Training: Batch 62/1146. Loss of 0.9506, cover loss of 0.2077, secret loss of 0.9906\n",
            "Training: Batch 63/1146. Loss of 1.1213, cover loss of 0.1536, secret loss of 1.2902\n",
            "Training: Batch 64/1146. Loss of 1.3491, cover loss of 0.2389, secret loss of 1.4802\n",
            "Training: Batch 65/1146. Loss of 1.4877, cover loss of 0.3364, secret loss of 1.5351\n",
            "Training: Batch 66/1146. Loss of 1.3686, cover loss of 0.2846, secret loss of 1.4453\n",
            "Training: Batch 67/1146. Loss of 0.9886, cover loss of 0.1814, secret loss of 1.0762\n",
            "Training: Batch 68/1146. Loss of 1.0752, cover loss of 0.1837, secret loss of 1.1887\n",
            "Training: Batch 69/1146. Loss of 1.3930, cover loss of 0.2313, secret loss of 1.5490\n",
            "Training: Batch 70/1146. Loss of 0.9706, cover loss of 0.2818, secret loss of 0.9184\n",
            "Training: Batch 71/1146. Loss of 1.6697, cover loss of 0.2934, secret loss of 1.8350\n",
            "Training: Batch 72/1146. Loss of 1.0184, cover loss of 0.1811, secret loss of 1.1163\n",
            "Training: Batch 73/1146. Loss of 1.2110, cover loss of 0.1555, secret loss of 1.4073\n",
            "Training: Batch 74/1146. Loss of 1.2760, cover loss of 0.3158, secret loss of 1.2803\n",
            "Training: Batch 75/1146. Loss of 1.0499, cover loss of 0.1599, secret loss of 1.1867\n",
            "Training: Batch 76/1146. Loss of 1.2223, cover loss of 0.1909, secret loss of 1.3752\n",
            "Training: Batch 77/1146. Loss of 1.2710, cover loss of 0.3176, secret loss of 1.2711\n",
            "Training: Batch 78/1146. Loss of 0.7836, cover loss of 0.1907, secret loss of 0.7905\n",
            "Training: Batch 79/1146. Loss of 0.9803, cover loss of 0.2015, secret loss of 1.0383\n",
            "Training: Batch 80/1146. Loss of 1.2932, cover loss of 0.2960, secret loss of 1.3296\n",
            "Training: Batch 81/1146. Loss of 1.6833, cover loss of 0.6838, secret loss of 1.3328\n",
            "Training: Batch 82/1146. Loss of 1.3993, cover loss of 0.3517, secret loss of 1.3968\n",
            "Training: Batch 83/1146. Loss of 1.4246, cover loss of 0.2727, secret loss of 1.5359\n",
            "Training: Batch 84/1146. Loss of 1.1620, cover loss of 0.0772, secret loss of 1.4464\n",
            "Training: Batch 85/1146. Loss of 1.0057, cover loss of 0.3082, secret loss of 0.9300\n",
            "Training: Batch 86/1146. Loss of 1.3985, cover loss of 0.1264, secret loss of 1.6962\n",
            "Training: Batch 87/1146. Loss of 1.4401, cover loss of 0.1407, secret loss of 1.7325\n",
            "Training: Batch 88/1146. Loss of 1.0055, cover loss of 0.2716, secret loss of 0.9786\n",
            "Training: Batch 89/1146. Loss of 1.4154, cover loss of 0.2451, secret loss of 1.5603\n",
            "Training: Batch 90/1146. Loss of 1.0285, cover loss of 0.1337, secret loss of 1.1931\n",
            "Training: Batch 91/1146. Loss of 1.4092, cover loss of 0.2735, secret loss of 1.5143\n",
            "Training: Batch 92/1146. Loss of 1.0429, cover loss of 0.1583, secret loss of 1.1794\n",
            "Training: Batch 93/1146. Loss of 1.4079, cover loss of 0.1240, secret loss of 1.7119\n",
            "Training: Batch 94/1146. Loss of 0.8264, cover loss of 0.1710, secret loss of 0.8739\n",
            "Training: Batch 95/1146. Loss of 1.1980, cover loss of 0.2139, secret loss of 1.3122\n",
            "Training: Batch 96/1146. Loss of 1.4082, cover loss of 0.3822, secret loss of 1.3680\n",
            "Training: Batch 97/1146. Loss of 1.1059, cover loss of 0.1207, secret loss of 1.3136\n",
            "Training: Batch 98/1146. Loss of 1.2636, cover loss of 0.2312, secret loss of 1.3766\n",
            "Training: Batch 99/1146. Loss of 0.9027, cover loss of 0.3112, secret loss of 0.7887\n",
            "Training: Batch 100/1146. Loss of 1.1518, cover loss of 0.1680, secret loss of 1.3118\n",
            "Training: Batch 101/1146. Loss of 1.6492, cover loss of 0.3936, secret loss of 1.6740\n",
            "Training: Batch 102/1146. Loss of 0.8853, cover loss of 0.1517, secret loss of 0.9782\n",
            "Training: Batch 103/1146. Loss of 1.3527, cover loss of 0.1856, secret loss of 1.5561\n",
            "Training: Batch 104/1146. Loss of 1.3165, cover loss of 0.2343, secret loss of 1.4430\n",
            "Training: Batch 105/1146. Loss of 1.2424, cover loss of 0.3158, secret loss of 1.2355\n",
            "Training: Batch 106/1146. Loss of 1.3069, cover loss of 0.2437, secret loss of 1.4176\n",
            "Training: Batch 107/1146. Loss of 0.9178, cover loss of 0.1641, secret loss of 1.0050\n",
            "Training: Batch 108/1146. Loss of 1.2606, cover loss of 0.1271, secret loss of 1.5113\n",
            "Training: Batch 109/1146. Loss of 1.3353, cover loss of 0.2074, secret loss of 1.5038\n",
            "Training: Batch 110/1146. Loss of 1.2192, cover loss of 0.3521, secret loss of 1.1560\n",
            "Training: Batch 111/1146. Loss of 0.8691, cover loss of 0.1664, secret loss of 0.9369\n",
            "Training: Batch 112/1146. Loss of 0.9981, cover loss of 0.1777, secret loss of 1.0939\n",
            "Training: Batch 113/1146. Loss of 1.1199, cover loss of 0.1428, secret loss of 1.3027\n",
            "Training: Batch 114/1146. Loss of 1.0108, cover loss of 0.1831, secret loss of 1.1037\n",
            "Training: Batch 115/1146. Loss of 1.2043, cover loss of 0.2720, secret loss of 1.2430\n",
            "Training: Batch 116/1146. Loss of 1.2275, cover loss of 0.2195, secret loss of 1.3440\n",
            "Training: Batch 117/1146. Loss of 1.0613, cover loss of 0.1294, secret loss of 1.2426\n",
            "Training: Batch 118/1146. Loss of 0.6490, cover loss of 0.1685, secret loss of 0.6407\n",
            "Training: Batch 119/1146. Loss of 1.4286, cover loss of 0.1254, secret loss of 1.7375\n",
            "Training: Batch 120/1146. Loss of 1.2385, cover loss of 0.1557, secret loss of 1.4438\n",
            "Training: Batch 121/1146. Loss of 0.9113, cover loss of 0.1928, secret loss of 0.9580\n",
            "Training: Batch 122/1146. Loss of 1.0737, cover loss of 0.3739, secret loss of 0.9330\n",
            "Training: Batch 123/1146. Loss of 1.1683, cover loss of 0.3858, secret loss of 1.0434\n",
            "Training: Batch 124/1146. Loss of 1.1645, cover loss of 0.2606, secret loss of 1.2052\n",
            "Training: Batch 125/1146. Loss of 1.0337, cover loss of 0.3417, secret loss of 0.9226\n",
            "Training: Batch 126/1146. Loss of 0.9253, cover loss of 0.1249, secret loss of 1.0672\n",
            "Training: Batch 127/1146. Loss of 0.9052, cover loss of 0.1103, secret loss of 1.0599\n",
            "Training: Batch 128/1146. Loss of 0.8325, cover loss of 0.1199, secret loss of 0.9500\n",
            "Training: Batch 129/1146. Loss of 0.7556, cover loss of 0.2257, secret loss of 0.7065\n",
            "Training: Batch 130/1146. Loss of 0.9053, cover loss of 0.1224, secret loss of 1.0439\n",
            "Training: Batch 131/1146. Loss of 1.5436, cover loss of 0.1267, secret loss of 1.8892\n",
            "Training: Batch 132/1146. Loss of 1.1337, cover loss of 0.0947, secret loss of 1.3853\n",
            "Training: Batch 133/1146. Loss of 0.8184, cover loss of 0.1113, secret loss of 0.9428\n",
            "Training: Batch 134/1146. Loss of 1.0030, cover loss of 0.1337, secret loss of 1.1591\n",
            "Training: Batch 135/1146. Loss of 1.3518, cover loss of 0.3580, secret loss of 1.3251\n",
            "Training: Batch 136/1146. Loss of 1.5774, cover loss of 0.2853, secret loss of 1.7228\n",
            "Training: Batch 137/1146. Loss of 0.9478, cover loss of 0.1146, secret loss of 1.1109\n",
            "Training: Batch 138/1146. Loss of 0.8085, cover loss of 0.0744, secret loss of 0.9787\n",
            "Training: Batch 139/1146. Loss of 0.9479, cover loss of 0.1383, secret loss of 1.0794\n",
            "Training: Batch 140/1146. Loss of 0.6043, cover loss of 0.1404, secret loss of 0.6186\n",
            "Training: Batch 141/1146. Loss of 0.7977, cover loss of 0.1836, secret loss of 0.8189\n",
            "Training: Batch 142/1146. Loss of 0.8508, cover loss of 0.1114, secret loss of 0.9859\n",
            "Training: Batch 143/1146. Loss of 1.2168, cover loss of 0.1406, secret loss of 1.4350\n",
            "Training: Batch 144/1146. Loss of 1.2177, cover loss of 0.1249, secret loss of 1.4571\n",
            "Training: Batch 145/1146. Loss of 1.1507, cover loss of 0.1305, secret loss of 1.3603\n",
            "Training: Batch 146/1146. Loss of 1.0555, cover loss of 0.1168, secret loss of 1.2515\n",
            "Training: Batch 147/1146. Loss of 1.2719, cover loss of 0.1638, secret loss of 1.4775\n",
            "Training: Batch 148/1146. Loss of 1.4129, cover loss of 0.1466, secret loss of 1.6884\n",
            "Training: Batch 149/1146. Loss of 1.6409, cover loss of 0.0755, secret loss of 2.0872\n",
            "Training: Batch 150/1146. Loss of 0.8350, cover loss of 0.0742, secret loss of 1.0144\n",
            "Training: Batch 151/1146. Loss of 1.1640, cover loss of 0.1108, secret loss of 1.4042\n",
            "Training: Batch 152/1146. Loss of 0.8161, cover loss of 0.0855, secret loss of 0.9741\n",
            "Training: Batch 153/1146. Loss of 0.6516, cover loss of 0.0923, secret loss of 0.7457\n",
            "Training: Batch 154/1146. Loss of 1.4087, cover loss of 0.1643, secret loss of 1.6592\n",
            "Training: Batch 155/1146. Loss of 1.0851, cover loss of 0.0942, secret loss of 1.3213\n",
            "Training: Batch 156/1146. Loss of 0.9669, cover loss of 0.0890, secret loss of 1.1705\n",
            "Training: Batch 157/1146. Loss of 1.1455, cover loss of 0.0912, secret loss of 1.4058\n",
            "Training: Batch 158/1146. Loss of 1.5192, cover loss of 0.1179, secret loss of 1.8685\n",
            "Training: Batch 159/1146. Loss of 0.9434, cover loss of 0.1183, secret loss of 1.1002\n",
            "Training: Batch 160/1146. Loss of 1.0154, cover loss of 0.0573, secret loss of 1.2774\n",
            "Training: Batch 161/1146. Loss of 0.9854, cover loss of 0.2284, secret loss of 1.0093\n",
            "Training: Batch 162/1146. Loss of 0.8752, cover loss of 0.0487, secret loss of 1.1021\n",
            "Training: Batch 163/1146. Loss of 0.8697, cover loss of 0.0582, secret loss of 1.0820\n",
            "Training: Batch 164/1146. Loss of 1.2583, cover loss of 0.1297, secret loss of 1.5047\n",
            "Training: Batch 165/1146. Loss of 0.8158, cover loss of 0.0611, secret loss of 1.0063\n",
            "Training: Batch 166/1146. Loss of 0.9818, cover loss of 0.0972, secret loss of 1.1795\n",
            "Training: Batch 167/1146. Loss of 0.8690, cover loss of 0.0831, secret loss of 1.0478\n",
            "Training: Batch 168/1146. Loss of 0.7184, cover loss of 0.0740, secret loss of 0.8592\n",
            "Training: Batch 169/1146. Loss of 0.8858, cover loss of 0.1482, secret loss of 0.9835\n",
            "Training: Batch 170/1146. Loss of 0.8653, cover loss of 0.2898, secret loss of 0.7673\n",
            "Training: Batch 171/1146. Loss of 0.6914, cover loss of 0.1286, secret loss of 0.7504\n",
            "Training: Batch 172/1146. Loss of 1.1345, cover loss of 0.0913, secret loss of 1.3910\n",
            "Training: Batch 173/1146. Loss of 1.1218, cover loss of 0.0628, secret loss of 1.4119\n",
            "Training: Batch 174/1146. Loss of 0.8718, cover loss of 0.1917, secret loss of 0.9067\n",
            "Training: Batch 175/1146. Loss of 0.7813, cover loss of 0.0835, secret loss of 0.9303\n",
            "Training: Batch 176/1146. Loss of 0.9465, cover loss of 0.0635, secret loss of 1.1773\n",
            "Training: Batch 177/1146. Loss of 0.6705, cover loss of 0.0530, secret loss of 0.8233\n",
            "Training: Batch 178/1146. Loss of 1.0451, cover loss of 0.0918, secret loss of 1.2711\n",
            "Training: Batch 179/1146. Loss of 0.5784, cover loss of 0.0454, secret loss of 0.7106\n",
            "Training: Batch 180/1146. Loss of 1.1087, cover loss of 0.0804, secret loss of 1.3711\n",
            "Training: Batch 181/1146. Loss of 1.0172, cover loss of 0.0953, secret loss of 1.2293\n",
            "Training: Batch 182/1146. Loss of 0.8180, cover loss of 0.0845, secret loss of 0.9780\n",
            "Training: Batch 183/1146. Loss of 0.8029, cover loss of 0.0661, secret loss of 0.9824\n",
            "Training: Batch 184/1146. Loss of 0.8070, cover loss of 0.0493, secret loss of 1.0102\n",
            "Training: Batch 185/1146. Loss of 0.9356, cover loss of 0.0274, secret loss of 1.2110\n",
            "Training: Batch 186/1146. Loss of 0.6381, cover loss of 0.0693, secret loss of 0.7585\n",
            "Training: Batch 187/1146. Loss of 1.1994, cover loss of 0.0976, secret loss of 1.4691\n",
            "Training: Batch 188/1146. Loss of 0.8387, cover loss of 0.0348, secret loss of 1.0718\n",
            "Training: Batch 189/1146. Loss of 1.4157, cover loss of 0.0555, secret loss of 1.8137\n",
            "Training: Batch 190/1146. Loss of 1.0566, cover loss of 0.1427, secret loss of 1.2185\n",
            "Training: Batch 191/1146. Loss of 0.8366, cover loss of 0.0656, secret loss of 1.0279\n",
            "Training: Batch 192/1146. Loss of 1.0883, cover loss of 0.0956, secret loss of 1.3235\n",
            "Training: Batch 193/1146. Loss of 0.9093, cover loss of 0.0290, secret loss of 1.1738\n",
            "Training: Batch 194/1146. Loss of 0.7533, cover loss of 0.0721, secret loss of 0.9083\n",
            "Training: Batch 195/1146. Loss of 0.9182, cover loss of 0.1302, secret loss of 1.0507\n",
            "Training: Batch 196/1146. Loss of 1.1535, cover loss of 0.0687, secret loss of 1.4465\n",
            "Training: Batch 197/1146. Loss of 1.1212, cover loss of 0.0702, secret loss of 1.4014\n",
            "Training: Batch 198/1146. Loss of 0.6829, cover loss of 0.0750, secret loss of 0.8105\n",
            "Training: Batch 199/1146. Loss of 1.0670, cover loss of 0.0494, secret loss of 1.3568\n",
            "Training: Batch 200/1146. Loss of 1.0287, cover loss of 0.0655, secret loss of 1.2842\n",
            "Training: Batch 201/1146. Loss of 0.9891, cover loss of 0.0545, secret loss of 1.2462\n",
            "Training: Batch 202/1146. Loss of 0.6503, cover loss of 0.0463, secret loss of 0.8054\n",
            "Training: Batch 203/1146. Loss of 0.9836, cover loss of 0.0698, secret loss of 1.2184\n",
            "Training: Batch 204/1146. Loss of 1.0100, cover loss of 0.2220, secret loss of 1.0507\n",
            "Training: Batch 205/1146. Loss of 0.9194, cover loss of 0.0800, secret loss of 1.1193\n",
            "Training: Batch 206/1146. Loss of 0.9299, cover loss of 0.1221, secret loss of 1.0771\n",
            "Training: Batch 207/1146. Loss of 1.1290, cover loss of 0.1204, secret loss of 1.3448\n",
            "Training: Batch 208/1146. Loss of 0.9457, cover loss of 0.0435, secret loss of 1.2028\n",
            "Training: Batch 209/1146. Loss of 0.8430, cover loss of 0.0280, secret loss of 1.0867\n",
            "Training: Batch 210/1146. Loss of 0.9959, cover loss of 0.0879, secret loss of 1.2107\n",
            "Training: Batch 211/1146. Loss of 0.9689, cover loss of 0.0607, secret loss of 1.2109\n",
            "Training: Batch 212/1146. Loss of 1.1479, cover loss of 0.1019, secret loss of 1.3947\n",
            "Training: Batch 213/1146. Loss of 1.2530, cover loss of 0.0493, secret loss of 1.6050\n",
            "Training: Batch 214/1146. Loss of 1.0525, cover loss of 0.0535, secret loss of 1.3320\n",
            "Training: Batch 215/1146. Loss of 0.9812, cover loss of 0.0421, secret loss of 1.2522\n",
            "Training: Batch 216/1146. Loss of 1.1288, cover loss of 0.0671, secret loss of 1.4155\n",
            "Training: Batch 217/1146. Loss of 1.0941, cover loss of 0.0504, secret loss of 1.3916\n",
            "Training: Batch 218/1146. Loss of 0.6464, cover loss of 0.0940, secret loss of 0.7366\n",
            "Training: Batch 219/1146. Loss of 0.8418, cover loss of 0.1128, secret loss of 0.9720\n",
            "Training: Batch 220/1146. Loss of 1.3210, cover loss of 0.1522, secret loss of 1.5584\n",
            "Training: Batch 221/1146. Loss of 0.9631, cover loss of 0.0424, secret loss of 1.2277\n",
            "Training: Batch 222/1146. Loss of 1.2156, cover loss of 0.0500, secret loss of 1.5541\n",
            "Training: Batch 223/1146. Loss of 0.6865, cover loss of 0.0706, secret loss of 0.8212\n",
            "Training: Batch 224/1146. Loss of 0.7415, cover loss of 0.0601, secret loss of 0.9085\n",
            "Training: Batch 225/1146. Loss of 1.0381, cover loss of 0.0528, secret loss of 1.3137\n",
            "Training: Batch 226/1146. Loss of 1.1661, cover loss of 0.0734, secret loss of 1.4569\n",
            "Training: Batch 227/1146. Loss of 0.9054, cover loss of 0.0417, secret loss of 1.1516\n",
            "Training: Batch 228/1146. Loss of 1.3816, cover loss of 0.1146, secret loss of 1.6893\n",
            "Training: Batch 229/1146. Loss of 1.2327, cover loss of 0.0575, secret loss of 1.5669\n",
            "Training: Batch 230/1146. Loss of 0.7627, cover loss of 0.0355, secret loss of 0.9696\n",
            "Training: Batch 231/1146. Loss of 1.0104, cover loss of 0.0387, secret loss of 1.2956\n",
            "Training: Batch 232/1146. Loss of 1.3062, cover loss of 0.0534, secret loss of 1.6703\n",
            "Training: Batch 233/1146. Loss of 0.9708, cover loss of 0.0702, secret loss of 1.2008\n",
            "Training: Batch 234/1146. Loss of 1.0333, cover loss of 0.0695, secret loss of 1.2851\n",
            "Training: Batch 235/1146. Loss of 0.9830, cover loss of 0.1123, secret loss of 1.1609\n",
            "Training: Batch 236/1146. Loss of 0.8017, cover loss of 0.0607, secret loss of 0.9879\n",
            "Training: Batch 237/1146. Loss of 1.0345, cover loss of 0.0369, secret loss of 1.3301\n",
            "Training: Batch 238/1146. Loss of 0.8838, cover loss of 0.0809, secret loss of 1.0705\n",
            "Training: Batch 239/1146. Loss of 0.9300, cover loss of 0.0550, secret loss of 1.1667\n",
            "Training: Batch 240/1146. Loss of 0.8713, cover loss of 0.0496, secret loss of 1.0956\n",
            "Training: Batch 241/1146. Loss of 1.4245, cover loss of 0.0577, secret loss of 1.8224\n",
            "Training: Batch 242/1146. Loss of 0.9772, cover loss of 0.0999, secret loss of 1.1697\n",
            "Training: Batch 243/1146. Loss of 0.8020, cover loss of 0.0425, secret loss of 1.0128\n",
            "Training: Batch 244/1146. Loss of 0.9661, cover loss of 0.0926, secret loss of 1.1647\n",
            "Training: Batch 245/1146. Loss of 1.2444, cover loss of 0.0459, secret loss of 1.5981\n",
            "Training: Batch 246/1146. Loss of 0.9594, cover loss of 0.0505, secret loss of 1.2119\n",
            "Training: Batch 247/1146. Loss of 0.7295, cover loss of 0.0329, secret loss of 0.9288\n",
            "Training: Batch 248/1146. Loss of 0.9195, cover loss of 0.0565, secret loss of 1.1507\n",
            "Training: Batch 249/1146. Loss of 0.9700, cover loss of 0.0385, secret loss of 1.2420\n",
            "Training: Batch 250/1146. Loss of 0.8377, cover loss of 0.0330, secret loss of 1.0729\n",
            "Training: Batch 251/1146. Loss of 0.8138, cover loss of 0.0300, secret loss of 1.0451\n",
            "Training: Batch 252/1146. Loss of 0.6838, cover loss of 0.0392, secret loss of 0.8595\n",
            "Training: Batch 253/1146. Loss of 0.8537, cover loss of 0.0358, secret loss of 1.0905\n",
            "Training: Batch 254/1146. Loss of 0.8125, cover loss of 0.0682, secret loss of 0.9925\n",
            "Training: Batch 255/1146. Loss of 0.8496, cover loss of 0.0348, secret loss of 1.0864\n",
            "Training: Batch 256/1146. Loss of 1.3042, cover loss of 0.0444, secret loss of 1.6798\n",
            "Training: Batch 257/1146. Loss of 0.9925, cover loss of 0.1149, secret loss of 1.1701\n",
            "Training: Batch 258/1146. Loss of 1.0155, cover loss of 0.0659, secret loss of 1.2660\n",
            "Training: Batch 259/1146. Loss of 1.2810, cover loss of 0.0910, secret loss of 1.5867\n",
            "Training: Batch 260/1146. Loss of 0.9487, cover loss of 0.0672, secret loss of 1.1753\n",
            "Training: Batch 261/1146. Loss of 0.9543, cover loss of 0.0856, secret loss of 1.1582\n",
            "Training: Batch 262/1146. Loss of 1.2500, cover loss of 0.0455, secret loss of 1.6060\n",
            "Training: Batch 263/1146. Loss of 0.5672, cover loss of 0.0611, secret loss of 0.6748\n",
            "Training: Batch 264/1146. Loss of 1.1707, cover loss of 0.0308, secret loss of 1.5199\n",
            "Training: Batch 265/1146. Loss of 1.0344, cover loss of 0.0463, secret loss of 1.3175\n",
            "Training: Batch 266/1146. Loss of 0.8259, cover loss of 0.0663, secret loss of 1.0128\n",
            "Training: Batch 267/1146. Loss of 0.9676, cover loss of 0.0765, secret loss of 1.1881\n",
            "Training: Batch 268/1146. Loss of 0.8166, cover loss of 0.0415, secret loss of 1.0334\n",
            "Training: Batch 269/1146. Loss of 0.7533, cover loss of 0.0318, secret loss of 0.9620\n",
            "Training: Batch 270/1146. Loss of 1.0219, cover loss of 0.0359, secret loss of 1.3146\n",
            "Training: Batch 271/1146. Loss of 0.5878, cover loss of 0.0736, secret loss of 0.6855\n",
            "Training: Batch 272/1146. Loss of 0.9074, cover loss of 0.0513, secret loss of 1.1415\n",
            "Training: Batch 273/1146. Loss of 0.8033, cover loss of 0.0459, secret loss of 1.0099\n",
            "Training: Batch 274/1146. Loss of 1.0039, cover loss of 0.0786, secret loss of 1.2338\n",
            "Training: Batch 275/1146. Loss of 0.9728, cover loss of 0.0415, secret loss of 1.2416\n",
            "Training: Batch 276/1146. Loss of 1.6645, cover loss of 0.0562, secret loss of 2.1445\n",
            "Training: Batch 277/1146. Loss of 0.7330, cover loss of 0.0459, secret loss of 0.9161\n",
            "Training: Batch 278/1146. Loss of 1.0588, cover loss of 0.0308, secret loss of 1.3706\n",
            "Training: Batch 279/1146. Loss of 0.7872, cover loss of 0.0386, secret loss of 0.9982\n",
            "Training: Batch 280/1146. Loss of 0.8843, cover loss of 0.0776, secret loss of 1.0756\n",
            "Training: Batch 281/1146. Loss of 0.7397, cover loss of 0.0566, secret loss of 0.9108\n",
            "Training: Batch 282/1146. Loss of 0.9017, cover loss of 0.0668, secret loss of 1.1132\n",
            "Training: Batch 283/1146. Loss of 0.6459, cover loss of 0.0211, secret loss of 0.8330\n",
            "Training: Batch 284/1146. Loss of 0.9570, cover loss of 0.0200, secret loss of 1.2493\n",
            "Training: Batch 285/1146. Loss of 0.9425, cover loss of 0.0305, secret loss of 1.2161\n",
            "Training: Batch 286/1146. Loss of 1.2241, cover loss of 0.0217, secret loss of 1.6033\n",
            "Training: Batch 287/1146. Loss of 0.8942, cover loss of 0.0334, secret loss of 1.1478\n",
            "Training: Batch 288/1146. Loss of 0.8480, cover loss of 0.0303, secret loss of 1.0902\n",
            "Training: Batch 289/1146. Loss of 1.2668, cover loss of 0.0460, secret loss of 1.6277\n",
            "Training: Batch 290/1146. Loss of 0.8305, cover loss of 0.0328, secret loss of 1.0636\n",
            "Training: Batch 291/1146. Loss of 1.3862, cover loss of 0.0576, secret loss of 1.7715\n",
            "Training: Batch 292/1146. Loss of 0.8737, cover loss of 0.0305, secret loss of 1.1243\n",
            "Training: Batch 293/1146. Loss of 0.9423, cover loss of 0.0391, secret loss of 1.2042\n",
            "Training: Batch 294/1146. Loss of 1.3233, cover loss of 0.0578, secret loss of 1.6872\n",
            "Training: Batch 295/1146. Loss of 0.9223, cover loss of 0.0577, secret loss of 1.1527\n",
            "Training: Batch 296/1146. Loss of 1.0121, cover loss of 0.0412, secret loss of 1.2946\n",
            "Training: Batch 297/1146. Loss of 0.8230, cover loss of 0.0356, secret loss of 1.0500\n",
            "Training: Batch 298/1146. Loss of 0.8408, cover loss of 0.0300, secret loss of 1.0811\n",
            "Training: Batch 299/1146. Loss of 1.2868, cover loss of 0.0383, secret loss of 1.6647\n",
            "Training: Batch 300/1146. Loss of 0.7394, cover loss of 0.1034, secret loss of 0.8480\n",
            "Training: Batch 301/1146. Loss of 0.8701, cover loss of 0.0447, secret loss of 1.1004\n",
            "Training: Batch 302/1146. Loss of 0.9452, cover loss of 0.0636, secret loss of 1.1755\n",
            "Training: Batch 303/1146. Loss of 0.8279, cover loss of 0.0302, secret loss of 1.0636\n",
            "Training: Batch 304/1146. Loss of 1.0749, cover loss of 0.0279, secret loss of 1.3960\n",
            "Training: Batch 305/1146. Loss of 1.3219, cover loss of 0.0437, secret loss of 1.7042\n",
            "Training: Batch 306/1146. Loss of 0.6539, cover loss of 0.0473, secret loss of 0.8088\n",
            "Training: Batch 307/1146. Loss of 0.9434, cover loss of 0.0301, secret loss of 1.2177\n",
            "Training: Batch 308/1146. Loss of 1.0845, cover loss of 0.2072, secret loss of 1.1697\n",
            "Training: Batch 309/1146. Loss of 0.9021, cover loss of 0.0451, secret loss of 1.1427\n",
            "Training: Batch 310/1146. Loss of 0.9972, cover loss of 0.0622, secret loss of 1.2468\n",
            "Training: Batch 311/1146. Loss of 1.0887, cover loss of 0.0733, secret loss of 1.3539\n",
            "Training: Batch 312/1146. Loss of 0.8969, cover loss of 0.0262, secret loss of 1.1609\n",
            "Training: Batch 313/1146. Loss of 1.5313, cover loss of 0.0315, secret loss of 1.9998\n",
            "Training: Batch 314/1146. Loss of 1.0377, cover loss of 0.0369, secret loss of 1.3343\n",
            "Training: Batch 315/1146. Loss of 1.2680, cover loss of 0.0461, secret loss of 1.6291\n",
            "Training: Batch 316/1146. Loss of 0.8902, cover loss of 0.0425, secret loss of 1.1303\n",
            "Training: Batch 317/1146. Loss of 1.8186, cover loss of 0.0589, secret loss of 2.3463\n",
            "Training: Batch 318/1146. Loss of 1.1764, cover loss of 0.0294, secret loss of 1.5294\n",
            "Training: Batch 319/1146. Loss of 1.1206, cover loss of 0.0530, secret loss of 1.4235\n",
            "Training: Batch 320/1146. Loss of 0.6342, cover loss of 0.0349, secret loss of 0.7990\n",
            "Training: Batch 321/1146. Loss of 0.5479, cover loss of 0.0336, secret loss of 0.6857\n",
            "Training: Batch 322/1146. Loss of 0.6694, cover loss of 0.0411, secret loss of 0.8377\n",
            "Training: Batch 323/1146. Loss of 1.0686, cover loss of 0.0792, secret loss of 1.3191\n",
            "Training: Batch 324/1146. Loss of 0.9207, cover loss of 0.0198, secret loss of 1.2013\n",
            "Training: Batch 325/1146. Loss of 0.8614, cover loss of 0.0377, secret loss of 1.0983\n",
            "Training: Batch 326/1146. Loss of 0.7690, cover loss of 0.0360, secret loss of 0.9774\n",
            "Training: Batch 327/1146. Loss of 0.9963, cover loss of 0.0792, secret loss of 1.2229\n",
            "Training: Batch 328/1146. Loss of 0.9981, cover loss of 0.0172, secret loss of 1.3078\n",
            "Training: Batch 329/1146. Loss of 0.9374, cover loss of 0.0453, secret loss of 1.1895\n",
            "Training: Batch 330/1146. Loss of 1.1664, cover loss of 0.1016, secret loss of 1.4197\n",
            "Training: Batch 331/1146. Loss of 1.0601, cover loss of 0.0419, secret loss of 1.3576\n",
            "Training: Batch 332/1146. Loss of 0.9813, cover loss of 0.0134, secret loss of 1.2905\n",
            "Training: Batch 333/1146. Loss of 1.1941, cover loss of 0.0292, secret loss of 1.5531\n",
            "Training: Batch 334/1146. Loss of 1.1353, cover loss of 0.0303, secret loss of 1.4733\n",
            "Training: Batch 335/1146. Loss of 0.8357, cover loss of 0.0303, secret loss of 1.0738\n",
            "Training: Batch 336/1146. Loss of 1.2783, cover loss of 0.0444, secret loss of 1.6451\n",
            "Training: Batch 337/1146. Loss of 0.9819, cover loss of 0.0514, secret loss of 1.2406\n",
            "Training: Batch 338/1146. Loss of 1.0689, cover loss of 0.0265, secret loss of 1.3900\n",
            "Training: Batch 339/1146. Loss of 0.6300, cover loss of 0.0313, secret loss of 0.7982\n",
            "Training: Batch 340/1146. Loss of 0.6741, cover loss of 0.0142, secret loss of 0.8799\n",
            "Training: Batch 341/1146. Loss of 1.0496, cover loss of 0.0204, secret loss of 1.3723\n",
            "Training: Batch 342/1146. Loss of 1.1234, cover loss of 0.0353, secret loss of 1.4508\n",
            "Training: Batch 343/1146. Loss of 0.6914, cover loss of 0.0189, secret loss of 0.8966\n",
            "Training: Batch 344/1146. Loss of 0.8686, cover loss of 0.0292, secret loss of 1.1191\n",
            "Training: Batch 345/1146. Loss of 0.9622, cover loss of 0.0245, secret loss of 1.2503\n",
            "Training: Batch 346/1146. Loss of 0.9008, cover loss of 0.0225, secret loss of 1.1710\n",
            "Training: Batch 347/1146. Loss of 0.8604, cover loss of 0.0187, secret loss of 1.1223\n",
            "Training: Batch 348/1146. Loss of 1.2827, cover loss of 0.0556, secret loss of 1.6360\n",
            "Training: Batch 349/1146. Loss of 1.0512, cover loss of 0.0315, secret loss of 1.3597\n",
            "Training: Batch 350/1146. Loss of 0.7491, cover loss of 0.0343, secret loss of 0.9530\n",
            "Training: Batch 351/1146. Loss of 0.6143, cover loss of 0.0474, secret loss of 0.7559\n",
            "Training: Batch 352/1146. Loss of 1.1835, cover loss of 0.0168, secret loss of 1.5556\n",
            "Training: Batch 353/1146. Loss of 0.8896, cover loss of 0.0439, secret loss of 1.1276\n",
            "Training: Batch 354/1146. Loss of 0.7903, cover loss of 0.0117, secret loss of 1.0381\n",
            "Training: Batch 355/1146. Loss of 1.2111, cover loss of 0.0201, secret loss of 1.5881\n",
            "Training: Batch 356/1146. Loss of 0.8591, cover loss of 0.0260, secret loss of 1.1108\n",
            "Training: Batch 357/1146. Loss of 1.1466, cover loss of 0.0189, secret loss of 1.5037\n",
            "Training: Batch 358/1146. Loss of 1.0375, cover loss of 0.0573, secret loss of 1.3070\n",
            "Training: Batch 359/1146. Loss of 0.9917, cover loss of 0.0244, secret loss of 1.2898\n",
            "Training: Batch 360/1146. Loss of 1.0157, cover loss of 0.0475, secret loss of 1.2910\n",
            "Training: Batch 361/1146. Loss of 1.0328, cover loss of 0.0593, secret loss of 1.2981\n",
            "Training: Batch 362/1146. Loss of 0.9769, cover loss of 0.0326, secret loss of 1.2591\n",
            "Training: Batch 363/1146. Loss of 0.7993, cover loss of 0.0098, secret loss of 1.0526\n",
            "Training: Batch 364/1146. Loss of 0.7326, cover loss of 0.0275, secret loss of 0.9401\n",
            "Training: Batch 365/1146. Loss of 0.8301, cover loss of 0.0123, secret loss of 1.0904\n",
            "Training: Batch 366/1146. Loss of 0.5800, cover loss of 0.0228, secret loss of 0.7429\n",
            "Training: Batch 367/1146. Loss of 0.8950, cover loss of 0.0361, secret loss of 1.1451\n",
            "Training: Batch 368/1146. Loss of 0.8283, cover loss of 0.0243, secret loss of 1.0719\n",
            "Training: Batch 369/1146. Loss of 0.8794, cover loss of 0.0148, secret loss of 1.1528\n",
            "Training: Batch 370/1146. Loss of 1.1113, cover loss of 0.0273, secret loss of 1.4453\n",
            "Training: Batch 371/1146. Loss of 1.4551, cover loss of 0.0133, secret loss of 1.9225\n",
            "Training: Batch 372/1146. Loss of 1.3452, cover loss of 0.0263, secret loss of 1.7585\n",
            "Training: Batch 373/1146. Loss of 1.0841, cover loss of 0.0291, secret loss of 1.4067\n",
            "Training: Batch 374/1146. Loss of 0.7889, cover loss of 0.0150, secret loss of 1.0319\n",
            "Training: Batch 375/1146. Loss of 0.7687, cover loss of 0.0238, secret loss of 0.9932\n",
            "Training: Batch 376/1146. Loss of 1.1986, cover loss of 0.0302, secret loss of 1.5578\n",
            "Training: Batch 377/1146. Loss of 0.6768, cover loss of 0.0181, secret loss of 0.8783\n",
            "Training: Batch 378/1146. Loss of 0.8465, cover loss of 0.0168, secret loss of 1.1063\n",
            "Training: Batch 379/1146. Loss of 1.1546, cover loss of 0.0296, secret loss of 1.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ2VP7EcJiVy"
      },
      "source": [
        "# Plot loss through epochs\n",
        "plt.plot(loss_history)\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Batch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-80UGqlaJib0"
      },
      "source": [
        "# net.load_state_dict(torch.load(MODELS_PATH+'Epoch N4.pkl'))\n",
        "\n",
        "# Switch to evaluate mode\n",
        "net.eval()\n",
        "\n",
        "test_losses = []\n",
        "# Show images\n",
        "for idx, test_batch in enumerate(test_loader):\n",
        "     # Saves images\n",
        "    data, _ = test_batch\n",
        "\n",
        "    # Saves secret images and secret covers\n",
        "    test_secret = data[:len(data)//2]\n",
        "    test_cover = data[len(data)//2:]\n",
        "\n",
        "    # Creates variable from secret and cover images\n",
        "    test_secret = Variable(test_secret, volatile=True)\n",
        "    test_cover = Variable(test_cover, volatile=True)\n",
        "\n",
        "    # Compute output\n",
        "    test_hidden, test_output = net(test_secret, test_cover)\n",
        "    \n",
        "    # Calculate loss\n",
        "    test_loss, loss_cover, loss_secret = customized_loss(test_output, test_hidden, test_secret, test_cover, beta)\n",
        "    \n",
        "#     diff_S, diff_C = np.abs(np.array(test_output.data[0]) - np.array(test_secret.data[0])), np.abs(np.array(test_hidden.data[0]) - np.array(test_cover.data[0]))\n",
        "    \n",
        "#     print (diff_S, diff_C)\n",
        "    \n",
        "    if idx in [1,2,3,4]:\n",
        "        print ('Total loss: {:.2f} \\nLoss on secret: {:.2f} \\nLoss on cover: {:.2f}'.format(test_loss.item(), loss_secret.item(), loss_cover.item()))\n",
        "\n",
        "        # Creates img tensor\n",
        "        imgs = [test_secret.data, test_output.data, test_cover.data, test_hidden.data]\n",
        "        imgs_tsor = torch.cat(imgs, 0)\n",
        "\n",
        "        # Prints Images\n",
        "        imshow(utils.make_grid(imgs_tsor), idx+1, learning_rate=learning_rate, beta=beta)\n",
        "        \n",
        "    test_losses.append(test_loss.item())\n",
        "        \n",
        "mean_test_loss = np.mean(test_losses)\n",
        "\n",
        "print ('Average loss on test set: {:.2f}'.format(mean_test_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
